# Review of WWW 2026 Paper #551: ShardBenchmark

**Paper Title:** ShardBenchmark: LLM-Driven Address Placement and Account Migration in Sharding Blockchains

---

## Summary

This paper presents ShardBenchmark, a benchmark and simulator for evaluating Address Placement (AP) and Account Migration (AM) policies in sharded blockchains. The authors formalize AP, AM, and their joint coordination (AP+AM) as constrained sequential decision-making tasks modeled as MDPs. Drawing on empirical patterns from Ethereum data, they construct three controllable synthetic datasets (M-Realish, M-Middle, M-Stress) that capture head-heavy activity distributions, temporal drift, and community clustering. The paper proposes an LLM-driven decision framework that compresses shard-state observations into textual prompts and outputs structured actions with auditable rationales. Experimental results indicate that stronger LLMs generally achieve higher rewards than the reported baselines on the proposed metrics, with the unified AP+AM setting achieving the best performance.

---

## Strengths

**S1. Novel application domain.** To the best of my knowledge, this is among the first systematic works applying LLMs to blockchain sharding schedulers, unifying AP and AM under a single decision-making framework. The connection between constrained sequential optimization and prompt-driven LLM inference is creative and well-motivated.

**S2. Unified benchmark design.** The paper treats AP, AM, and AP+AM as three distinct but related tasks within a single evaluation pipeline. This unified framing enables direct comparison of policies across different control mechanisms and reveals how placement and migration interact—a coupling that prior work largely studied in isolation.

**S3. Controllable synthetic datasets.** The three datasets (M-Realish, M-Middle, M-Stress) systematically vary stickiness, head-effect concentration, and reuse probability, allowing for controlled ablation of algorithmic sensitivity to different traffic patterns. The head-effect calibration procedure (Eq. 13) and community-driven receiver model (Eqs. 9–10) are technically sound and grounded in empirical Ethereum statistics.

**S4. Comprehensive evaluation scope.** The experiments cover multiple state-of-the-art LLMs (DeepSeek-R1, GPT-4.1, Claude, Gemini, Qwen3 variants) across three tasks and three datasets, with consistent prompt templates and structured-output enforcement. The results clearly demonstrate scale effects (Qwen3-8B vs 13B vs 32B) and reasoning quality differences.

**S5. Interpretable decision framework.** Unlike black-box heuristics or trained RL policies, the LLM-driven approach produces auditable rationales alongside actions. The case study in Section 5.3 and Appendix D effectively illustrates how DeepSeek-R1 reasons over shard statistics and balances competing objectives.

**S6. Thorough documentation and reproducibility support.** The paper provides detailed notation (Appendix A), hyperparameter tables (Appendix B), algorithmic pseudocode (Appendices C), and a prompt-response example (Appendix D). The level of detail regarding reward functions, state-space construction, and synthetic data generation supports reproducibility and facilitates future extensions.

---

## Weaknesses

**Overview of main concerns:**
My primary reservations fall into three groups: **(i) lack of real-trace validation**, **(ii) weak baselines and missing statistical rigor**, and **(iii) incomplete system-level evaluation** (deployment cost, latency, scalability). Below I elaborate on each.

### **(i) Lack of Real-Trace Validation**

**W1. Synthetic-only evaluation.**
All experiments rely exclusively on synthetic datasets generated by the authors' Lognormal + community model. While the model is grounded in Ethereum statistics, no direct comparison with real blockchain traces is provided. This raises two issues:
- **Causal interpretability:** Real traces contain exogenous factors (e.g., protocol upgrades, flash-loan attacks, NFT drops) that synthetic data may not capture. Without at least a small-scale real-trace experiment or a synthetic-vs-real comparison on summary statistics, it is unclear how tightly the conclusions transfer to production environments.
- **Generalization risk:** If the LLM learned to exploit artifacts specific to the Lognormal generator (e.g., fixed community structures, deterministic hotspot amplification), performance on live data may degrade.

**Suggestion:** Add a small pilot study on real Ethereum or BSC transaction logs, or at minimum compare key distributional properties (degree distribution, temporal auto-correlation) between synthetic and real data.

### **(ii) Weak Baselines and Missing Statistical Analysis**

**W2. Limited baseline comparisons.**
The primary baseline is a Random policy, which sets a very low bar. The PPO implementation is mentioned as a control but reportedly "does not exhibit clear improvements over Random" due to lack of a dedicated training phase (Section 5.1). The Least-Loaded (LL) heuristic shows only marginal gains. Critically, the paper omits comparisons with prior published methods:
- **Spring (Li et al., WWW'24):** Uses deep RL for AP and reports substantial gains over hash-based placement.
- **AERO (Song et al., WWW'25):** Proposes RL-driven AM with specialized data structures.

Without head-to-head comparisons (even if approximated via reproduced implementations or author-reported numbers), it is difficult to assess whether LLM-driven policies represent a genuine advance or simply exceed an artificially weak baseline.

**W3. Absence of statistical significance testing.**
Tables 2–4 report point estimates without variance or confidence intervals. Given the stochastic nature of both the synthetic data generator and (potentially) the LLM sampling process, single-run numbers are insufficient to support claims of "consistent outperformance." Standard practice would include multiple independent seeds and paired t-tests or bootstrap confidence intervals.

**Suggestion:** (a) Implement or cite performance numbers from Spring/AERO; (b) report mean ± std over at least 5 runs with different seeds; (c) provide statistical tests where differences are claimed.

### **(iii) Incomplete System-Level Evaluation**

**W4. Missing cost and latency analysis.**
The paper does not measure or discuss:
- **LLM inference cost:** API pricing for models like GPT-4.1 or DeepSeek-R1 can be substantial. How many API calls are required per 1000 blocks? What is the dollar cost for a realistic workload?
- **Latency:** Blockchain block times (e.g., 12s for Ethereum) impose strict decision deadlines. How long does prompt construction + LLM query + parsing take? Is it feasible to invoke the LLM every block for AP, or would one need caching / batching?
- **Throughput overhead:** Migration operations (AM) incur state-transfer costs. The paper tracks cross-shard ratio and load variance but does not quantify actual TPS degradation during migration windows.

**W5. Limited scalability and sensitivity analysis.**
- **Shard count:** All experiments fix K=16. How do LLM policies scale to K=64 or K=256? Does prompt length become prohibitive?
- **Migration budget:** The paper fixes migration period P=100 and max migrations a=5. Sensitivity to these hyperparameters is not explored. For instance, does doubling the migration frequency improve reward monotonically, or is there a saturation/overhead trade-off?
- **Reward-function weights:** α=β=1 throughout (Table 6). How robust are rankings to different α/β settings (e.g., prioritizing load balance over cross-shard ratio)?

**Suggestion:** (a) Add a cost-benefit table (reward gain vs. API cost); (b) measure end-to-end decision latency; (c) include small-scale ablations over K, P, a, α/β to clarify when LLM advantages hold.

---

## Technical Quality: **Good**

The core MDP formulations (Sections 2.1–2.3), synthetic data generation model (Section 3), and LLM prompt construction (Section 4) are technically sound and carefully implemented. The reward function (Eqs. 3–5) multiplicatively combines cross-shard efficiency and load balance, which is reasonable though somewhat ad-hoc (alternative formulations, e.g., constrained optimization with hard load caps, are not discussed). The action parameterization—proportion vectors for AP (Eq. 17) and migration pairs for AM (Eq. 19)—is a sensible design choice that reduces the combinatorial action space.

However, the weaknesses noted above—particularly the absence of real-trace validation, limited baseline comparisons, and missing cost/latency analysis—prevent a higher rating. These gaps do not invalidate the benchmark design, but they do limit the strength of the empirical claims. Despite these missing pieces, the overall benchmark design and LLM integration appear technically solid and the experimental methodology is transparent.

---

## Presentation Quality: **Excellent**

The paper is well-structured, clearly written, and easy to follow. Notation is consistent and systematically summarized in Appendix A. The three-task decomposition (AP, AM, AP+AM) provides a logical progression, and the experimental section (Section 5) presents results in well-organized tables and figures. Figure 1 effectively illustrates the overall workflow, and Figure 4 clearly decomposes the AP vs AM vs AP+AM trade-offs across metrics. The case study (Section 5.3) and full prompt-response example (Appendix D) are particularly helpful for understanding how the LLM reasons in practice. Appendices B and C provide comprehensive hyperparameter settings and algorithmic pseudocode, which enhance reproducibility. Minor improvements could include more discussion of failure modes (e.g., when smaller LLMs produce infeasible outputs) and clearer guidance on when to use which dataset variant.

---

## Expected Impact: **Broad**

**Positive contributions:**
- **Benchmark artifact:** If released and maintained as promised, ShardBenchmark could serve as a standard testbed for subsequent AP/AM and LLM-scheduler research, similar to how RL benchmarks (e.g., OpenAI Gym) have catalyzed progress in reinforcement learning.
- **Methodological template:** The prompt-engineering and action-parameterization strategies may transfer to other blockchain control problems (e.g., validator selection, fee markets, MEV mitigation) or even to non-blockchain distributed systems.
- **Cross-disciplinary bridge:** The paper connects blockchain systems, sequential decision-making, and LLM reasoning in a novel way, which may inspire follow-on work in both the blockchain and AI communities.

**Limitations:**
- **Deployment readiness:** Without real-trace validation and cost/latency analysis, it remains unclear whether LLM-driven scheduling is practically deployable in latency-sensitive, production-grade sharded blockchains.
- **Generalization:** The synthetic datasets, while carefully designed, may not fully capture adversarial or black-swan scenarios (e.g., flash-loan cascades, coordinated Sybil attacks).

Overall, the benchmark and datasets have the potential to enable systematic comparisons and reproducible research, which would be valuable for the community even if the specific LLM policies require further refinement for production use.

---

## Suggestions for Improvement

I consolidate my recommendations into four actionable groups:

**R1. Real-trace validation and synthetic-vs-real comparison**
Add at least a small-scale pilot experiment on real blockchain data (e.g., Ethereum mainnet or a public testnet). Compare key distributional properties—degree distribution, temporal drift, community structure—between the synthetic datasets and real traces. If full real-trace evaluation is infeasible for the camera-ready, explicitly discuss this limitation and outline how future work could validate the synthetic model.

**R2. Stronger baselines and statistical rigor**
- Implement or compare against prior published methods (Spring, AERO) to establish a more meaningful performance baseline.
- Report results as mean ± standard deviation over multiple independent runs (at least 5 seeds).
- Provide statistical significance tests (e.g., paired t-tests, bootstrap CIs) to support claims of "consistent" or "significant" improvements.

**R3. System-level evaluation: cost, latency, and deployment feasibility**
- **Cost analysis:** Estimate API costs (or local inference time for open models) per 1000 blocks. Provide a cost-benefit table: reward gain vs. dollar cost or compute budget.
- **Latency profiling:** Measure end-to-end decision time (prompt construction + LLM query + parsing) and discuss whether it fits within typical block intervals.
- **Throughput metrics:** Quantify actual TPS or block processing time, especially during migration windows, to assess whether AM overhead is acceptable.
- **Scalability study:** Test with larger K (e.g., 64 or 128 shards) and vary hyperparameters (P, a, α/β) to identify regimes where LLM advantages hold or degrade.

**R4. Robustness, sensitivity, and failure-mode analysis**
- Expand discussion of when smaller LLMs fail to produce valid outputs (currently mentioned only in passing for Qwen3-4B). What fraction of queries result in format errors or constraint violations?
- Ablate reward-function weights (α, β) and migration parameters (P, a) to clarify sensitivity.
- Discuss potential failure modes: e.g., what happens if the LLM hallucinates invalid shard IDs, or if prompt length exceeds context limits for very large K?

*Note on scope:* I do not expect the camera-ready to exhaustively cover all these points, but addressing at least R1 (pilot real-trace study or explicit limitation discussion), R2 (basic statistical reporting), and partial R3 (cost/latency estimates) would substantially strengthen the empirical claims and practical relevance.

---

## Overall Evaluation: **Weak Accept**

This paper makes a solid contribution by introducing ShardBenchmark, a unified and reproducible testbed for AP and AM in sharded blockchains, and by demonstrating that LLM-driven policies can outperform simple baselines on carefully designed synthetic workloads. The benchmark design is technically sound, the experimental methodology is transparent, and the presentation is clear. The connection between blockchain scheduling and LLM reasoning is novel and may inspire future work.

However, several limitations temper my enthusiasm:
1. **Lack of real-trace validation:** All results are on synthetic data, with no direct comparison to real blockchain logs or distributional validation against live traffic.
2. **Weak baselines:** Comparisons are primarily against Random and an untrained PPO, with no head-to-head benchmarks against prior published methods (Spring, AERO).
3. **Missing system-level analysis:** No quantification of LLM inference cost, latency, or actual throughput impact, which are critical for assessing production feasibility.

These gaps prevent a higher rating (Accept or Strong Accept), but they do not fundamentally undermine the benchmark's value as a research tool. I therefore recommend **Weak Accept**: the paper merits publication contingent on (a) clearer acknowledgment of the synthetic-only evaluation as a limitation and (b) ideally, at least preliminary cost/latency estimates or a commitment to real-trace follow-up work.

If the authors can address points R1–R3 in rebuttal—either by adding a small real-trace pilot or by providing detailed cost/latency measurements—I would be happy to raise my score.

---

## Confidence: **Medium (3/5)**

I am familiar with blockchain sharding, MDP formulations, and LLM prompt engineering, and I have reviewed the technical content carefully. However, I am not a domain expert in production blockchain systems or large-scale distributed consensus protocols. My assessment of the synthetic data model's realism and the practical deployment challenges may be incomplete. I would defer to a reviewer with deeper blockchain engineering experience for final judgment on the real-world viability of the LLM-driven approach, particularly regarding cost, latency, and adversarial robustness.

---

## Confidential Remarks for Program Committee

This is a well-executed piece of work that I am inclined to support, but I want to highlight three items for the AC and fellow reviewers:

1. **Real-trace gap:** The lack of any real blockchain data is the most significant weakness. While the synthetic model is carefully designed, I believe at least a small-scale Ethereum trace experiment (or explicit comparison of statistical properties) is necessary to validate the generative assumptions. If other reviewers share this concern, we should push the authors on this point during rebuttal.

2. **Baseline strength:** The Random and untrained-PPO baselines are quite weak. Ideally, the paper would compare against Spring (WWW'24) or AERO (WWW'25). If the authors argue that reimplementing these methods is infeasible, they should at least discuss qualitative differences and provide rough performance estimates from the cited papers.

3. **Production feasibility:** The paper does not quantify LLM inference cost or latency, which may substantially limit the feasibility of deploying the current approach in latency-sensitive, production-grade sharded systems. For a benchmark paper, this is less critical than for a systems paper, but it would strengthen the practical impact if addressed.

4. **Reviewer expertise:** My background is stronger in ML/RL and decision-making than in blockchain engineering. I recommend assigning a co-reviewer with deep expertise in sharded blockchain protocols (e.g., Ethereum 2.0, Polkadot, Zilliqa) to verify the realism of the MDP formulation, reward functions, and migration constraints.

**Bottom line:** I half-champion this paper. It introduces a useful benchmark and demonstrates an interesting LLM application, but it needs modest strengthening (real-trace pilot or clearer limitation discussion + cost/latency estimates) to be truly compelling. If the authors respond constructively in rebuttal, I am prepared to advocate for acceptance.

---

## Questions for Authors (Rebuttal)

1. **Real-trace validation:** Can you provide even a small pilot experiment on real Ethereum or BSC data, or at minimum compare distributional properties (e.g., degree distribution, clustering coefficient) between your synthetic datasets and actual traces?

2. **Baseline comparisons:** Why were Spring (WWW'24) and AERO (WWW'25) not included as baselines? Can you provide at least qualitative comparisons or rough performance estimates from those papers?

3. **Cost and latency:** What is the estimated API cost (or local inference time) per 1000 blocks for DeepSeek-R1 and GPT-4.1? What is the end-to-end decision latency (prompt + query + parse)? Is this compatible with typical blockchain block times?

4. **Statistical significance:** Can you report mean ± std over multiple runs and provide significance tests for the main comparisons in Tables 2–4?

5. **Scalability:** How do LLM policies perform with K=64 or K=128 shards? Does prompt length become prohibitive?

---

**Recommendation:** Weak Accept
**Confidence:** Medium (3/5)
